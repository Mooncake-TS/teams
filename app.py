# app.py
# =========================================
# Streamlit - LSTM ê°ì •ë¶„ë¥˜ (Review.xlsx ê¸°ë°˜)
# - ì…ë ¥: Review.xlsx (columns: Sentiment, Review)
# - í•™ìŠµ: Tokenizer + Embedding + (Bi)LSTM
# - ê¸°ëŠ¥: (1) ë°ì´í„° ë¡œë“œ/ë¯¸ë¦¬ë³´ê¸° (2) ë²„íŠ¼ìœ¼ë¡œ í•™ìŠµ (3) ì…ë ¥ -> ì˜ˆì¸¡
# =========================================

import re
import random
import numpy as np
import pandas as pd
import streamlit as st

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import layers, models


# ----------------------------
# 0) Streamlit ì„¤ì •
# ----------------------------
st.set_page_config(page_title="ë¦¬ë·° ê°ì •ë¶„ì„ (LSTM)", layout="wide")

DEFAULT_XLSX_PATH = "data/Review.xlsx"  # ë¦¬í¬ì§€í† ë¦¬ì— data/Review.xlsx ë„£ìœ¼ë©´ ìë™ ë¡œë“œ

# ê°ë…ê´€ ìš”êµ¬ ë¶ˆìš©ì–´(ì˜ˆì‹œ) + "ë„ˆë¬´"
BASE_STOPWORDS = set([
    "ì´", "ê°€", "ì„", "ë¥¼", "ì€", "ëŠ”", "ì—", "ì—ì„œ", "ì—ê²Œ",
    "ì˜", "ì™€", "ê³¼", "ë„", "ë¡œ", "ìœ¼ë¡œ",
    "í•˜ë‹¤", "ë˜ë‹¤", "ìˆë‹¤", "ì—†ë‹¤",
    "ê·¸", "ì €", "ê²ƒ", "ìˆ˜",
    "ì¢€", "ì˜", "ë§¤ìš°", "ì •ë§",
    "ë•Œë¬¸", "ê°™ë‹¤",
    "ë„ˆë¬´"
])


# ----------------------------
# 1) ìœ í‹¸
# ----------------------------
def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)


def clean_text(s: str) -> str:
    s = str(s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def simple_tokenize(s: str, stopwords: set) -> list[str]:
    """
    konlpy ì—†ì´ë„ ëŒì•„ê°€ê²Œ ë§Œë“  ì´ˆê°„ë‹¨ í† í°í™”:
    - í•œê¸€/ì˜ë¬¸/ìˆ«ì ì™¸ ë¬¸ì ì œê±°
    - ê³µë°± split
    - ë¶ˆìš©ì–´ ì œê±°
    """
    s = clean_text(s)
    s = re.sub(r"[^0-9a-zA-Zê°€-í£\s]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    tokens = s.split()
    tokens = [t for t in tokens if (t not in stopwords and len(t) > 1)]
    return tokens


def preprocess_to_string(s: str, stopwords: set) -> str:
    return " ".join(simple_tokenize(s, stopwords))


@st.cache_data(show_spinner=False)
def load_review_xlsx_from_path(path: str) -> pd.DataFrame | None:
    try:
        df = pd.read_excel(path, sheet_name=0)
        return df
    except Exception:
        return None


def load_review_xlsx(uploaded_file=None) -> pd.DataFrame | None:
    # 1) ì—…ë¡œë“œê°€ ìˆìœ¼ë©´ ê·¸ê±¸ ìµœìš°ì„ 
    if uploaded_file is not None:
        try:
            return pd.read_excel(uploaded_file, sheet_name=0)
        except Exception:
            return None
    # 2) ì—†ìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ ì‹œë„
    return load_review_xlsx_from_path(DEFAULT_XLSX_PATH)


def build_model(vocab_size: int, max_len: int, num_classes: int,
                emb_dim=128, lstm_units=64, dropout=0.3) -> tf.keras.Model:
    model = models.Sequential([
        layers.Input(shape=(max_len,)),
        layers.Embedding(input_dim=vocab_size, output_dim=emb_dim, mask_zero=True),
        layers.Bidirectional(layers.LSTM(lstm_units)),
        layers.Dropout(dropout),
        layers.Dense(64, activation="relu"),
        layers.Dropout(dropout),
        layers.Dense(num_classes, activation="softmax")
    ])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"]
    )
    return model


def train_lstm_pipeline(
    df: pd.DataFrame,
    stopwords: set,
    vocab_size: int,
    max_len: int,
    epochs: int,
    batch_size: int,
    test_size: float,
    seed: int
):
    set_seed(seed)

    # í•„ìˆ˜ ì»¬ëŸ¼
    required_cols = {"Sentiment", "Review"}
    missing = required_cols - set(df.columns)
    if missing:
        raise ValueError(f"ì—‘ì…€ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ì–´: {missing}. í˜„ì¬ ì»¬ëŸ¼: {list(df.columns)}")

    work = df[["Sentiment", "Review"]].copy()
    work["Sentiment"] = work["Sentiment"].astype(str).fillna("").str.strip()
    work["Review"] = work["Review"].astype(str).fillna("").str.strip()
    work = work[(work["Sentiment"] != "") & (work["Review"] != "")].copy()

    # ì „ì²˜ë¦¬
    texts_raw = work["Review"].tolist()
    labels_raw = work["Sentiment"].tolist()
    texts = [preprocess_to_string(t, stopwords) for t in texts_raw]

    # ë¼ë²¨ ì¸ì½”ë”©
    le = LabelEncoder()
    y = le.fit_transform(labels_raw)

    # split (ë¼ë²¨ ë¶„í¬ ìœ ì§€)
    X_train, X_test, y_train, y_test = train_test_split(
        texts, y,
        test_size=float(test_size),
        random_state=int(seed),
        stratify=y
    )

    # Tokenizer + Padding
    tokenizer = Tokenizer(num_words=int(vocab_size), oov_token="<OOV>")
    tokenizer.fit_on_texts(X_train)

    def to_pad(x_list):
        seq = tokenizer.texts_to_sequences(x_list)
        return pad_sequences(seq, maxlen=int(max_len), padding="post", truncating="post")

    X_train_pad = to_pad(X_train)
    X_test_pad = to_pad(X_test)

    # ëª¨ë¸
    num_classes = len(le.classes_)
    model = build_model(vocab_size=int(vocab_size), max_len=int(max_len), num_classes=num_classes)

    callbacks = [
        tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=2, restore_best_weights=True)
    ]

    history = model.fit(
        X_train_pad, y_train,
        validation_split=0.2,
        epochs=int(epochs),
        batch_size=int(batch_size),
        callbacks=callbacks,
        verbose=0
    )

    # í‰ê°€
    probs = model.predict(X_test_pad, verbose=0)
    pred = np.argmax(probs, axis=1)

    acc = float(accuracy_score(y_test, pred))
    report = classification_report(y_test, pred, target_names=le.classes_, digits=4)
    cm = confusion_matrix(y_test, pred)

    # ìƒ˜í”Œ ëª‡ê°œ ë½‘ì•„ì„œ ì˜ˆì¸¡ ë³´ê¸°
    sample_idx = np.random.choice(len(X_test), size=min(6, len(X_test)), replace=False)
    samples = []
    for i in sample_idx:
        txt = X_test[i]
        true_label = le.inverse_transform([y_test[i]])[0]
        pred_label = le.inverse_transform([pred[i]])[0]
        samples.append((true_label, pred_label, txt[:140] + ("..." if len(txt) > 140 else "")))

    return {
        "model": model,
        "tokenizer": tokenizer,
        "label_encoder": le,
        "max_len": int(max_len),
        "metrics": {
            "accuracy": acc,
            "report": report,
            "confusion_matrix": cm,
            "classes": list(le.classes_),
            "samples": samples,
            "history": history.history,
            "label_dist": work["Sentiment"].value_counts().to_dict(),
        }
    }


def predict_one(text: str, stopwords: set, model, tokenizer, le, max_len: int):
    proc = preprocess_to_string(text, stopwords)
    seq = tokenizer.texts_to_sequences([proc])
    pad = pad_sequences(seq, maxlen=max_len, padding="post", truncating="post")
    probs = model.predict(pad, verbose=0)[0]
    idx = int(np.argmax(probs))
    label = le.inverse_transform([idx])[0]
    prob_dict = {le.classes_[i]: float(probs[i]) for i in range(len(le.classes_))}
    return label, float(np.max(probs)), prob_dict, proc


# ----------------------------
# 2) UI
# ----------------------------
st.title("ë¦¬ë·° ê°ì • ë¶„ì„ (LSTM)")
st.caption("Review.xlsxì˜ Sentiment/Reviewë¡œ í•™ìŠµí•˜ê³ , ì•„ë˜ ì…ë ¥ì°½ì—ì„œ ë°”ë¡œ ì˜ˆì¸¡í•©ë‹ˆë‹¤.")

with st.sidebar:
    st.header("1) ë°ì´í„°")
    uploaded = st.file_uploader("Review.xlsx ì—…ë¡œë“œ(ì„ íƒ)", type=["xlsx"])
    st.caption("ì—…ë¡œë“œ ì•ˆ í•˜ë©´ ë¦¬í¬ì§€í† ë¦¬ì˜ data/Review.xlsxë¥¼ ìë™ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤.")

    st.header("2) í•™ìŠµ ì„¤ì •")
    vocab_size = st.number_input("VOCAB_SIZE", 5000, 50000, 20000, step=1000)
    max_len = st.number_input("MAX_LEN", 20, 300, 120, step=10)
    epochs = st.number_input("epochs", 1, 50, 8, step=1)
    batch_size = st.selectbox("batch_size", [8, 16, 32, 64], index=1)
    test_size = st.slider("test_size", 0.1, 0.4, 0.2, 0.05)
    seed = st.number_input("seed", 0, 9999, 42, step=1)

    st.header("3) ë¶ˆìš©ì–´")
    extra_sw = st.text_input("ì¶”ê°€ ë¶ˆìš©ì–´(ì‰¼í‘œë¡œ êµ¬ë¶„)", value="")

    st.divider()
    train_btn = st.button("ğŸš€ í•™ìŠµ ì‹œì‘", use_container_width=True)

# stopwords í™•ì •
STOPWORDS = set(BASE_STOPWORDS)
if extra_sw.strip():
    for w in extra_sw.split(","):
        w = w.strip()
        if w:
            STOPWORDS.add(w)

# ë°ì´í„° ë¡œë“œ
df = load_review_xlsx(uploaded)
if df is None:
    st.error("Review.xlsxë¥¼ ëª» ì½ì—ˆì–´. (ì—…ë¡œë“œ íŒŒì¼/ data/Review.xlsx ê²½ë¡œ/ì—‘ì…€ ì†ìƒ ì—¬ë¶€ í™•ì¸)")
    st.stop()

# ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
required_cols = {"Sentiment", "Review"}
missing = required_cols - set(df.columns)
if missing:
    st.error(f"ì—‘ì…€ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ì–´: {missing}\ní˜„ì¬ ì»¬ëŸ¼: {list(df.columns)}")
    st.stop()

view_df = df[["Sentiment", "Review"]].copy()
view_df["Sentiment"] = view_df["Sentiment"].astype(str).fillna("").str.strip()
view_df["Review"] = view_df["Review"].astype(str).fillna("").str.strip()
view_df = view_df[(view_df["Sentiment"] != "") & (view_df["Review"] != "")].copy()

c1, c2 = st.columns([2, 1])
with c1:
    st.subheader("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
    st.dataframe(view_df.head(12), use_container_width=True)
with c2:
    st.subheader("ë¼ë²¨ ë¶„í¬")
    st.write(view_df["Sentiment"].value_counts())

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "trained" not in st.session_state:
    st.session_state.trained = False
    st.session_state.bundle = None

# í•™ìŠµ ë²„íŠ¼ ëˆŒë €ì„ ë•Œë§Œ í•™ìŠµ (í•µì‹¬!)
if train_btn:
    with st.spinner("í•™ìŠµ ì¤‘... (ì²˜ìŒ í•œ ë²ˆë§Œ ì¡°ê¸ˆ ê¸°ë‹¤ë¦¬ë©´ ë¨)"):
        try:
            bundle = train_lstm_pipeline(
                df=view_df,
                stopwords=STOPWORDS,
                vocab_size=int(vocab_size),
                max_len=int(max_len),
                epochs=int(epochs),
                batch_size=int(batch_size),
                test_size=float(test_size),
                seed=int(seed),
            )
            st.session_state.bundle = bundle
            st.session_state.trained = True
            st.success("í•™ìŠµ ì™„ë£Œ! ì•„ë˜ì—ì„œ ì„±ëŠ¥ í™•ì¸ + ì˜ˆì¸¡í•´ë´.")
        except Exception as e:
            st.session_state.trained = False
            st.session_state.bundle = None
            st.exception(e)

# í•™ìŠµ ê²°ê³¼ í‘œì‹œ
st.divider()
st.subheader("í•™ìŠµ/í‰ê°€ ê²°ê³¼")

if not st.session_state.trained:
    st.info("ì™¼ìª½ì—ì„œ **í•™ìŠµ ì‹œì‘**ì„ ëˆŒëŸ¬ì•¼ ì˜ˆì¸¡ì´ í™œì„±í™”ë¼.")
else:
    bundle = st.session_state.bundle
    m = bundle["metrics"]

    st.write(f"âœ… í…ŒìŠ¤íŠ¸ ì •í™•ë„: **{m['accuracy']:.4f}**")
    st.text("Classification Report:\n" + m["report"])

    cm = m["confusion_matrix"]
    cm_df = pd.DataFrame(cm, index=m["classes"], columns=m["classes"])
    st.write("Confusion Matrix")
    st.dataframe(cm_df, use_container_width=True)

    st.write("í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ì˜ˆì¸¡(ì¼ë¶€)")
    sample_df = pd.DataFrame(m["samples"], columns=["true", "pred", "text_preview"])
    st.dataframe(sample_df, use_container_width=True)

    st.divider()
    st.subheader("ë¦¬ë·° í•œ ì¤„ ì…ë ¥ â†’ ì˜ˆì¸¡")

    user_text = st.text_area("ë¦¬ë·°ë¥¼ ì…ë ¥í•´ì¤˜", value="", height=120, placeholder="ì˜ˆ) ë‹´ë°° ëƒ„ìƒˆê°€ ì‹¬í•˜ê³  ìš´ì „ì´ ê±°ì¹ ì—ˆì–´ìš”.")
    pred_btn = st.button("ğŸ”® ì˜ˆì¸¡í•˜ê¸°")

    if pred_btn:
        if not user_text.strip():
            st.warning("ë¦¬ë·°ë¥¼ ë¨¼ì € ì…ë ¥í•´ì¤˜!")
        else:
            label, conf, prob_dict, proc = predict_one(
                user_text,
                STOPWORDS,
                bundle["model"],
                bundle["tokenizer"],
                bundle["label_encoder"],
                bundle["max_len"],
            )
            st.success(f"ì˜ˆì¸¡: **{label}**  (confidence={conf:.3f})")
            st.write("í´ë˜ìŠ¤ë³„ í™•ë¥ ")
            st.json(prob_dict)
            with st.expander("ì „ì²˜ë¦¬ëœ ì…ë ¥(ëª¨ë¸ì— ë“¤ì–´ê°„ í…ìŠ¤íŠ¸) ë³´ê¸°"):
                st.code(proc)

st.caption("íŒ: ë°ì´í„°ê°€ ì ê±°ë‚˜ ë¼ë²¨ì´ ì• ë§¤í•˜ë©´(ì¤‘ë¦½/ë¶€ì • ê²½ê³„) ì •í™•ë„ê°€ ë“¤ì‘¥ë‚ ì‘¥í•  ìˆ˜ ìˆì–´. ê·¸ê±´ ë„¤ ì‹¤ë ¥ì´ ì•„ë‹ˆë¼ ë°ì´í„°ì˜ ë¬¼ë¦¬ë²•ì¹™ì´ì•¼ ğŸ˜µâ€ğŸ’«")
